{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import argparse\n",
        "from silver_control import get_last_processed_run_id, update_last_processed_run_id, get_new_run_ids\n",
        "\n",
        "\n",
        "def load_fact_patient_encounters_incremental(spark: SparkSession, catalog_name: str, run_ids: list[str]):\n",
        "    \"\"\"Load fact_patient_encounters table incrementally.\"\"\"\n",
        "    \n",
        "    run_ids_str = \"', '\".join(run_ids)\n",
        "    \n",
        "    df = spark.sql(f\"\"\"\n",
        "        SELECT\n",
        "            e.Id as encounter_id,\n",
        "            e.PATIENT as patient_id,\n",
        "            e.PROVIDER as provider_id,\n",
        "            e.ORGANIZATION as organization_id,\n",
        "            e.PAYER as payer_id,\n",
        "            e.START as encounter_start_timestamp,\n",
        "            e.STOP as encounter_stop_timestamp,\n",
        "            DATE(e.START) as encounter_date_key,\n",
        "            (unix_timestamp(e.STOP) - unix_timestamp(e.START)) / 60 as encounter_duration_minutes,\n",
        "            e.ENCOUNTERCLASS as encounter_class,\n",
        "            e.CODE as encounter_code,\n",
        "            e.DESCRIPTION as encounter_description,\n",
        "            e.REASONCODE as reason_code,\n",
        "            e.REASONDESCRIPTION as reason_description,\n",
        "            e.BASE_ENCOUNTER_COST as base_encounter_cost,\n",
        "            e.TOTAL_CLAIM_COST as total_claim_cost,\n",
        "            e.PAYER_COVERAGE as payer_coverage,\n",
        "            e.TOTAL_CLAIM_COST - e.PAYER_COVERAGE as patient_responsibility,\n",
        "            1 as encounter_count,\n",
        "            p.GENDER as patient_gender,\n",
        "            p.BIRTHDATE as patient_birthdate,\n",
        "            p.RACE as patient_race,\n",
        "            p.ETHNICITY as patient_ethnicity,\n",
        "            p.STATE as patient_state,\n",
        "            p.CITY as patient_city,\n",
        "            p.ZIP as patient_zip,\n",
        "            year(e.START) - year(p.BIRTHDATE) as patient_age_at_encounter,\n",
        "            prov.NAME as provider_name,\n",
        "            prov.SPECIALITY as provider_specialty,\n",
        "            prov.GENDER as provider_gender,\n",
        "            org.NAME as organization_name,\n",
        "            org.CITY as organization_city,\n",
        "            org.STATE as organization_state,\n",
        "            c.CODE as primary_diagnosis_code,\n",
        "            c.DESCRIPTION as primary_diagnosis_description,\n",
        "            e.ingest_run_id,\n",
        "            e.ingest_timestamp,\n",
        "            current_timestamp() as silver_load_timestamp\n",
        "        FROM {catalog_name}.synthea.encounters_bronze e\n",
        "        LEFT JOIN {catalog_name}.synthea.patients_bronze p \n",
        "            ON e.PATIENT = p.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.providers_bronze prov \n",
        "            ON e.PROVIDER = prov.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.organizations_bronze org \n",
        "            ON e.ORGANIZATION = org.Id\n",
        "        LEFT JOIN (\n",
        "            SELECT ENCOUNTER, CODE, DESCRIPTION,\n",
        "                   ROW_NUMBER() OVER (PARTITION BY ENCOUNTER ORDER BY START) as rn\n",
        "            FROM {catalog_name}.synthea.conditions_bronze\n",
        "        ) c ON e.Id = c.ENCOUNTER AND c.rn = 1\n",
        "        WHERE e.ingest_run_id IN ('{run_ids_str}')\n",
        "    \"\"\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "    # Get parameters from Databricks widgets (for notebook tasks)\n",
        "    try:\n",
        "        from pyspark.dbutils import DBUtils\n",
        "        dbutils = DBUtils(spark)\n",
        "        catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "    except Exception:\n",
        "        # Fallback to argparse for Python script tasks\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument(\"--catalog_name\", type=str, required=True)\n",
        "        args, _ = parser.parse_known_args()\n",
        "        catalog_name = args.catalog_name\n",
        "    table_name = \"patient_encounters_silver\"\n",
        "    \n",
        "    last_run_id = get_last_processed_run_id(spark, catalog_name, table_name)\n",
        "    print(f\"Last processed run ID: {last_run_id}\")\n",
        "    \n",
        "    new_run_ids = get_new_run_ids(spark, catalog_name, \"encounters_bronze\", last_run_id)\n",
        "    \n",
        "    if not new_run_ids:\n",
        "        print(\"No new data to process.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Processing {len(new_run_ids)} new run(s): {new_run_ids}\")\n",
        "    \n",
        "    df = load_fact_patient_encounters_incremental(spark, catalog_name, new_run_ids)\n",
        "    \n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.synthea\")\n",
        "    \n",
        "    (df.write\n",
        "        .mode(\"append\")\n",
        "        .format(\"delta\")\n",
        "        .option(\"mergeSchema\", \"true\")\n",
        "        .saveAsTable(f\"{catalog_name}.synthea.{table_name}\"))\n",
        "    \n",
        "    update_last_processed_run_id(spark, catalog_name, table_name, new_run_ids[-1])\n",
        "    \n",
        "    record_count = df.count()\n",
        "    print(f\"\u2713 Loaded {record_count} records into {table_name}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "databricks": {
      "environment": {
        "client": "4",
        "environmentId": "serverless_environment_demo"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "application/vnd.databricks.v1+notebook": {
      "environmentMetadata": {
        "base_environment": "",
        "dependencies": [
          "-r /Volumes/maggiedatabricksterraform_dbw/synthea/admin_configs/requirements.txt"
        ]
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}