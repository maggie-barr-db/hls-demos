{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import argparse\n",
        "from silver_control import get_last_processed_run_id, update_last_processed_run_id, get_new_run_ids, get_all_run_ids\n",
        "\n",
        "\n",
        "def load_fact_procedures_incremental(spark: SparkSession, catalog_name: str, run_ids: list[str]):\n",
        "    \"\"\"Load fact_procedures table incrementally.\"\"\"\n",
        "    \n",
        "    run_ids_str = \"', '\".join(run_ids)\n",
        "    \n",
        "    df = spark.sql(f\"\"\"\n",
        "        SELECT\n",
        "            md5(concat(pr.PATIENT, pr.ENCOUNTER, cast(pr.CODE as string), cast(pr.START as string))) as procedure_id,\n",
        "            pr.PATIENT as patient_id,\n",
        "            pr.ENCOUNTER as encounter_id,\n",
        "            pr.START as procedure_start_timestamp,\n",
        "            pr.STOP as procedure_stop_timestamp,\n",
        "            DATE(pr.START) as procedure_date_key,\n",
        "            (unix_timestamp(pr.STOP) - unix_timestamp(pr.START)) / 60 as procedure_duration_minutes,\n",
        "            pr.CODE as procedure_code,\n",
        "            pr.DESCRIPTION as procedure_description,\n",
        "            pr.SYSTEM as code_system,\n",
        "            pr.BASE_COST as procedure_cost,\n",
        "            pr.REASONCODE as reason_code,\n",
        "            pr.REASONDESCRIPTION as reason_description,\n",
        "            1 as procedure_count,\n",
        "            p.GENDER as patient_gender,\n",
        "            p.BIRTHDATE as patient_birthdate,\n",
        "            year(pr.START) - year(p.BIRTHDATE) as patient_age_at_procedure,\n",
        "            p.STATE as patient_state,\n",
        "            p.ZIP as patient_zip,\n",
        "            e.ENCOUNTERCLASS as encounter_class,\n",
        "            e.PROVIDER as provider_id,\n",
        "            e.ORGANIZATION as organization_id,\n",
        "            e.PAYER as payer_id,\n",
        "            prov.SPECIALITY as provider_specialty,\n",
        "            prov.NAME as provider_name,\n",
        "            org.NAME as organization_name,\n",
        "            org.CITY as organization_city,\n",
        "            org.STATE as organization_state,\n",
        "            pr.ingest_run_id,\n",
        "            pr.ingest_timestamp,\n",
        "            current_timestamp() as silver_load_timestamp\n",
        "        FROM {catalog_name}.synthea.procedures_bronze pr\n",
        "        LEFT JOIN {catalog_name}.synthea.patients_bronze p \n",
        "            ON pr.PATIENT = p.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.encounters_bronze e \n",
        "            ON pr.ENCOUNTER = e.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.providers_bronze prov \n",
        "            ON e.PROVIDER = prov.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.organizations_bronze org \n",
        "            ON e.ORGANIZATION = org.Id\n",
        "        WHERE pr.ingest_run_id IN ('{run_ids_str}')\n",
        "    \"\"\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "    # Get parameters from Databricks widgets (for notebook tasks)\n",
        "    try:\n",
        "        from pyspark.dbutils import DBUtils\n",
        "        dbutils = DBUtils(spark)\n",
        "        catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "        load_type = dbutils.widgets.get(\"load_type\") if dbutils.widgets.get(\"load_type\") else \"full\"\n",
        "    except Exception:\n",
        "        # Fallback to argparse for Python script tasks\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument(\"--catalog_name\", type=str, required=True)\n",
        "        parser.add_argument(\"--load_type\", type=str, default=\"full\", choices=[\"full\", \"incremental\"])\n",
        "        args, _ = parser.parse_known_args()\n",
        "        catalog_name = args.catalog_name\n",
        "        load_type = args.load_type\n",
        "    table_name = \"procedures_silver\"\n",
        "    \n",
        "    # Get run IDs to process based on load type\n",
        "    if load_type == \"full\":\n",
        "        print(\"Running FULL load - processing all data\")\n",
        "        run_ids = get_all_run_ids(spark, catalog_name, \"procedures_bronze\")\n",
        "    else:\n",
        "        # Incremental load\n",
        "        last_run_id = get_last_processed_run_id(spark, catalog_name, table_name)\n",
        "        print(f\"Running INCREMENTAL load - Last processed run ID: {last_run_id}\")\n",
        "        run_ids = get_new_run_ids(spark, catalog_name, \"procedures_bronze\", last_run_id)\n",
        "    \n",
        "    if not run_ids:\n",
        "        print(\"No data to process.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Processing {len(run_ids)} run(s): {run_ids}\")\n",
        "    \n",
        "    df = load_fact_procedures_incremental(spark, catalog_name, run_ids)\n",
        "    \n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.synthea\")\n",
        "    \n",
        "    # Write to silver table - use overwrite for full loads, append for incremental\n",
        "    write_mode = \"overwrite\" if load_type == \"full\" else \"append\"\n",
        "    (df.write\n",
        "        .mode(write_mode)\n",
        "        .format(\"delta\")\n",
        "        .option(\"mergeSchema\", \"true\")\n",
        "        .saveAsTable(f\"{catalog_name}.synthea.{table_name}\"))\n",
        "    \n",
        "    update_last_processed_run_id(spark, catalog_name, table_name, run_ids[-1])\n",
        "    \n",
        "    record_count = df.count()\n",
        "    print(f\"âœ“ Loaded {record_count} records into {table_name} (mode: {load_type})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "environmentMetadata": {
        "base_environment": "",
        "dependencies": [
          "-r /Volumes/maggiedatabricksterraform_dbw/synthea/admin_configs/requirements.txt"
        ]
      }
    },
    "databricks": {
      "environment": {
        "client": "4",
        "environmentId": "serverless_environment_demo"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
