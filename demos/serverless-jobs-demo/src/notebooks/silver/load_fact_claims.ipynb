{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import argparse\n",
        "from silver_control import get_last_processed_run_id, update_last_processed_run_id, get_new_run_ids\n",
        "\n",
        "\n",
        "def load_fact_claims_incremental(spark: SparkSession, catalog_name: str, run_ids: list[str]):\n",
        "    \"\"\"Load fact_claims table incrementally.\"\"\"\n",
        "    \n",
        "    # Filter for new run IDs\n",
        "    run_ids_str = \"', '\".join(run_ids)\n",
        "    \n",
        "    df = spark.sql(f\"\"\"\n",
        "        SELECT\n",
        "            ct.ID as claim_transaction_id,\n",
        "            ct.CLAIMID as claim_id,\n",
        "            ct.PATIENTID as patient_id,\n",
        "            ct.PROVIDERID as provider_id,\n",
        "            e.ORGANIZATION as organization_id,\n",
        "            e.PAYER as payer_id,\n",
        "            ct.FROMDATE as service_date,\n",
        "            ct.TODATE as service_end_date,\n",
        "            DATE(ct.FROMDATE) as service_date_key,\n",
        "            ct.TYPE as transaction_type,\n",
        "            ct.PROCEDURECODE as procedure_code,\n",
        "            ct.AMOUNT as claim_amount,\n",
        "            ct.UNITAMOUNT as unit_amount,\n",
        "            ct.UNITS as units,\n",
        "            ct.PAYMENTS as payment_amount,\n",
        "            ct.ADJUSTMENTS as adjustment_amount,\n",
        "            ct.TRANSFERS as transfer_amount,\n",
        "            ct.OUTSTANDING as outstanding_amount,\n",
        "            ct.PLACEOFSERVICE as place_of_service,\n",
        "            c.DIAGNOSIS1 as primary_diagnosis_code,\n",
        "            c.SERVICEDATE as claim_service_date,\n",
        "            c.STATUS1 as claim_status,\n",
        "            e.ENCOUNTERCLASS as encounter_class,\n",
        "            e.BASE_ENCOUNTER_COST as base_encounter_cost,\n",
        "            e.TOTAL_CLAIM_COST as total_claim_cost,\n",
        "            e.PAYER_COVERAGE as payer_coverage,\n",
        "            p.GENDER as patient_gender,\n",
        "            p.BIRTHDATE as patient_birthdate,\n",
        "            p.STATE as patient_state,\n",
        "            p.ZIP as patient_zip,\n",
        "            prov.SPECIALITY as provider_specialty,\n",
        "            prov.NAME as provider_name,\n",
        "            org.NAME as organization_name,\n",
        "            org.CITY as organization_city,\n",
        "            org.STATE as organization_state,\n",
        "            ct.ingest_run_id,\n",
        "            ct.ingest_timestamp,\n",
        "            current_timestamp() as silver_load_timestamp\n",
        "        FROM {catalog_name}.synthea.claims_transactions_bronze ct\n",
        "        LEFT JOIN {catalog_name}.synthea.claims_bronze c \n",
        "            ON ct.CLAIMID = c.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.encounters_bronze e \n",
        "            ON c.APPOINTMENTID = e.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.patients_bronze p \n",
        "            ON ct.PATIENTID = p.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.providers_bronze prov \n",
        "            ON ct.PROVIDERID = prov.Id\n",
        "        LEFT JOIN {catalog_name}.synthea.organizations_bronze org \n",
        "            ON e.ORGANIZATION = org.Id\n",
        "        WHERE ct.ingest_run_id IN ('{run_ids_str}')\n",
        "    \"\"\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "    # Get parameters from Databricks widgets (for notebook tasks)\n",
        "    try:\n",
        "        from pyspark.dbutils import DBUtils\n",
        "        dbutils = DBUtils(spark)\n",
        "        catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "    except Exception:\n",
        "        # Fallback to argparse for Python script tasks\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument(\"--catalog_name\", type=str, required=True)\n",
        "        args, _ = parser.parse_known_args()\n",
        "        catalog_name = args.catalog_name\n",
        "    table_name = \"claims_silver\"\n",
        "    \n",
        "    # Get last processed run ID\n",
        "    last_run_id = get_last_processed_run_id(spark, catalog_name, table_name)\n",
        "    print(f\"Last processed run ID: {last_run_id}\")\n",
        "    \n",
        "    # Get new run IDs to process\n",
        "    new_run_ids = get_new_run_ids(spark, catalog_name, \"claims_transactions_bronze\", last_run_id)\n",
        "    \n",
        "    if not new_run_ids:\n",
        "        print(\"No new data to process.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Processing {len(new_run_ids)} new run(s): {new_run_ids}\")\n",
        "    \n",
        "    # Load and write incremental data\n",
        "    df = load_fact_claims_incremental(spark, catalog_name, new_run_ids)\n",
        "    \n",
        "    # Create schema if needed\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.synthea\")\n",
        "    \n",
        "    # Write to silver table\n",
        "    (df.write\n",
        "        .mode(\"append\")\n",
        "        .format(\"delta\")\n",
        "        .option(\"mergeSchema\", \"true\")\n",
        "        .saveAsTable(f\"{catalog_name}.synthea.{table_name}\"))\n",
        "    \n",
        "    # Update control table with the latest run ID\n",
        "    update_last_processed_run_id(spark, catalog_name, table_name, new_run_ids[-1])\n",
        "    \n",
        "    record_count = df.count()\n",
        "    print(f\"\u2713 Loaded {record_count} records into {table_name}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "databricks": {
      "environment": {
        "client": "4",
        "environmentId": "serverless_environment_demo"
      }
    },
    "application/vnd.databricks.v1+notebook": {
      "environmentMetadata": {
        "base_environment": "",
        "dependencies": [
          "-r /Volumes/maggiedatabricksterraform_dbw/synthea/admin_configs/requirements.txt"
        ]
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}