{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bfddb9-82d6-490a-b4df-90e1ccfad798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FUNCTIONAL TESTING - Library & Configuration Demonstration\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Functional Testing Notebook\n",
    "# =============================================================================\n",
    "# This notebook demonstrates:\n",
    "# - Custom library usage (html2text)\n",
    "# - Job parameter configuration\n",
    "# - Spark configuration settings\n",
    "# - Environment variable configuration\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import argparse\n",
    "\n",
    "# Get Spark session\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FUNCTIONAL TESTING - Library & Configuration Demonstration\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b62f24-869a-4a54-ac2c-bd90921bc700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Configuration:\n",
      "  Catalog Name (Spark config):  maggiedatabricksterraform_dbw\n",
      "  Database Name (env var):      prod\n",
      "  Location (env var):           /Volumes/maggiedatabricksterraform_dbw/synthea/functional_testing\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Import Job Parameters and Environment Variables\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Get catalog_name from Spark config (set at cluster level)\n",
    "catalog_name = spark.conf.get(\"spark.databricks.sql.initial.catalog.name\")\n",
    "\n",
    "# Get database_name and location from cluster environment variables\n",
    "database_name = os.environ.get(\"DATABASE_NAME\")\n",
    "# Location points to /Volumes/maggiedatabricksterraform_dbw/synthea/functional_testing\n",
    "# This dedicated volume stores all files created during functional testing\n",
    "location = os.environ.get(\"LOCATION\")\n",
    "\n",
    "print(\"\\nJob Configuration:\")\n",
    "print(f\"  Catalog Name (Spark config):  {catalog_name}\")\n",
    "print(f\"  Database Name (env var):      {database_name}\")\n",
    "print(f\"  Location (env var):           {location}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2cd97c-4ce1-4b62-8a24-09b406cf3651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark Configuration (configured at cluster level):\n",
      "Displaying all 13 custom Spark configurations...\n",
      "\n",
      "  Initial Catalog.............................. maggiedatabricksterraform_dbw\n",
      "  Delta Preview Enabled........................ true\n",
      "  Data Lineage Enabled......................... true\n",
      "  SafeSpark External UDF Limit................. 25\n",
      "  Store Assignment Policy...................... LEGACY\n",
      "  Legacy Time Parser Policy.................... LEGACY\n",
      "  Parquet Int96 Rebase (Read).................. LEGACY\n",
      "  Parquet Int96 Rebase (Write)................. LEGACY\n",
      "  Parquet DateTime Rebase (Read)............... LEGACY\n",
      "  Parquet DateTime Rebase (Write).............. LEGACY\n",
      "  Network Timeout.............................. 800\n",
      "  Off-Heap Memory Enabled...................... false\n",
      "  Driver Extra Java Options.................... -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Xss32M\n",
      "\n",
      "✓ All Spark configurations are applied at cluster startup\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "FUNCTIONAL TESTS FOR SPARK CONFIGURATIONS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Display Spark Configuration (Set at Cluster Level)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nSpark Configuration (configured at cluster level):\")\n",
    "print(\"Displaying all 13 custom Spark configurations...\")\n",
    "\n",
    "# Display all configurations that are set in the cluster config\n",
    "configs_to_display = [\n",
    "    (\"Initial Catalog\", \"spark.databricks.sql.initial.catalog.name\"),\n",
    "    (\"Delta Preview Enabled\", \"spark.databricks.delta.preview.enabled\"),\n",
    "    (\"Data Lineage Enabled\", \"spark.databricks.dataLineage.enabled\"),\n",
    "    (\"SafeSpark External UDF Limit\", \"spark.databricks.safespark.externalUDF.plan.limit\"),\n",
    "    (\"Store Assignment Policy\", \"spark.sql.storeAssignmentPolicy\"),\n",
    "    (\"Legacy Time Parser Policy\", \"spark.sql.legacy.timeParserPolicy\"),\n",
    "    (\"Parquet Int96 Rebase (Read)\", \"spark.sql.parquet.int96RebaseModeInRead\"),\n",
    "    (\"Parquet Int96 Rebase (Write)\", \"spark.sql.parquet.int96RebaseModeInWrite\"),\n",
    "    (\"Parquet DateTime Rebase (Read)\", \"spark.sql.parquet.datetimeRebaseModeInRead\"),\n",
    "    (\"Parquet DateTime Rebase (Write)\", \"spark.sql.parquet.datetimeRebaseModeInWrite\"),\n",
    "    (\"Network Timeout\", \"spark.network.timeout\"),\n",
    "    (\"Off-Heap Memory Enabled\", \"spark.memory.offHeap.enabled\"),\n",
    "    (\"Driver Extra Java Options\", \"spark.driver.extraJavaOptions\")\n",
    "]\n",
    "\n",
    "print(\"\")\n",
    "for label, config_key in configs_to_display:\n",
    "    try:\n",
    "        value = spark.conf.get(config_key)\n",
    "        print(f\"  {label:.<45} {value}\")\n",
    "    except Exception:\n",
    "        print(f\"  {label:.<45} (not set)\")\n",
    "\n",
    "print(\"\\n✓ All Spark configurations are applied at cluster startup\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 3: Setup for Functional Tests\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType, StructType, StructField\n",
    "from datetime import datetime\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabdf4d0-a9d3-4507-ae78-121ff0f80d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 1: spark.databricks.sql.initial.catalog.name (synthea)\n",
      "======================================================================\n",
      "Current catalog: maggiedatabricksterraform_dbw\n",
      "✓ Can query without catalog prefix, initial catalog is: maggiedatabricksterraform_dbw\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: TEST 1 - Initial Catalog Name\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 1: spark.databricks.sql.initial.catalog.name (synthea)\")\n",
    "print(\"=\"*70)\n",
    "current_catalog = spark.catalog.currentCatalog()\n",
    "print(f\"Current catalog: {current_catalog}\")\n",
    "# Query without catalog prefix\n",
    "try:\n",
    "    # This should work because initial catalog is set to 'synthea'\n",
    "    test_query = f\"SHOW DATABASES\"\n",
    "    result = spark.sql(test_query)\n",
    "    print(f\"✓ Can query without catalog prefix, initial catalog is: {current_catalog}\")\n",
    "except Exception as e:\n",
    "    print(f\"Query result: {str(e)[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ef9fc1-cbc2-43f3-ac40-15b4beaf0712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 2: spark.sql.storeAssignmentPolicy (LEGACY)\n",
      "======================================================================\n",
      "Testing implicit type conversions with LEGACY policy...\n",
      "\n",
      "✓ LEGACY policy type coercion results:\n",
      "+---------------+-------------+\n",
      "|original_string|casted_to_int|\n",
      "+---------------+-------------+\n",
      "|123            |123          |\n",
      "|456            |456          |\n",
      "|not_a_number   |NULL         |\n",
      "|789            |789          |\n",
      "|invalid        |NULL         |\n",
      "+---------------+-------------+\n",
      "\n",
      "\n",
      "Observation:\n",
      "  - Valid strings ('123', '456', '789') converted to integers\n",
      "  - Invalid strings ('not_a_number', 'invalid') converted to NULL\n",
      "  - No errors thrown - this is the LEGACY behavior\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: TEST 2 - Store Assignment Policy (LEGACY)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: spark.sql.storeAssignmentPolicy (LEGACY)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing implicit type conversions with LEGACY policy...\\n\")\n",
    "\n",
    "try:\n",
    "    # Create DataFrame with string values\n",
    "    df_test = spark.createDataFrame([\n",
    "        (\"123\",),\n",
    "        (\"456\",),\n",
    "        (\"not_a_number\",),\n",
    "        (\"789\",),\n",
    "        (\"invalid\",)\n",
    "    ], [\"value_str\"])\n",
    "    \n",
    "    # Apply CAST - LEGACY policy should convert invalid values to NULL without errors\n",
    "    df_result = df_test.selectExpr(\n",
    "        \"value_str as original_string\",\n",
    "        \"CAST(value_str AS INT) as casted_to_int\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ LEGACY policy type coercion results:\")\n",
    "    df_result.show(truncate=False)\n",
    "    \n",
    "    print(\"\\nObservation:\")\n",
    "    print(\"  - Valid strings ('123', '456', '789') converted to integers\")\n",
    "    print(\"  - Invalid strings ('not_a_number', 'invalid') converted to NULL\")\n",
    "    print(\"  - No errors thrown - this is the LEGACY behavior\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Expected behavior with LEGACY policy: {str(e)[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbafb61-bb22-4426-8ce9-7da5d0481090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 3: spark.sql.legacy.timeParserPolicy (LEGACY)\n",
      "======================================================================\n",
      "✓ LEGACY time parser results:\n",
      "  to_date('2024-01-15') = 2024-01-15\n",
      "  to_timestamp('2024-01-15 14:30:00') = 2024-01-15 14:30:00\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: TEST 3 - Legacy Time Parser Policy\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 3: spark.sql.legacy.timeParserPolicy (LEGACY)\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing lenient date/time parsing with LEGACY policy...\\n\")\n",
    "\n",
    "try:\n",
    "    # LEGACY policy is more lenient with date formats and edge cases\n",
    "    # Create DataFrame with various date/time formats\n",
    "    df_dates = spark.createDataFrame([\n",
    "        (\"2024-01-15\",),           # Standard ISO format\n",
    "        (\"2024-01-15 14:30:00\",),  # With time\n",
    "        (\"15-01-2024\",),           # Day-first format\n",
    "        (\"01/15/2024\",),           # Slash separator\n",
    "        (\"2024-1-5\",),             # Single digit month/day\n",
    "        (\"invalid_date\",)          # Invalid format\n",
    "    ], [\"date_string\"])\n",
    "    \n",
    "    # Try parsing with LEGACY policy - it's more forgiving\n",
    "    df_parsed = df_dates.selectExpr(\n",
    "        \"date_string\",\n",
    "        \"to_date(date_string) as parsed_date\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ LEGACY time parser results:\")\n",
    "    df_parsed.show(truncate=False)\n",
    "    \n",
    "    print(\"\\nObservation:\")\n",
    "    print(\"  - LEGACY policy attempts to parse various date formats\")\n",
    "    print(\"  - Invalid formats result in NULL rather than throwing errors\")\n",
    "    print(\"  - More lenient than STRICT or CORRECTED policies\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Date parsing with LEGACY policy: {str(e)[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84d96b9-14e4-4a5d-ac93-306247cffce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 4: Parquet INT96 & DateTime Rebase Modes (LEGACY)\n",
      "======================================================================\n",
      "✓ Written Parquet with LEGACY rebasing\n",
      "✓ Read Parquet with LEGACY rebasing: 2 rows\n",
      "  int96RebaseModeInRead: LEGACY\n",
      "  int96RebaseModeInWrite: LEGACY\n",
      "  datetimeRebaseModeInRead: LEGACY\n",
      "  datetimeRebaseModeInWrite: LEGACY\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 7: TEST 4 - Parquet INT96 & DateTime Rebase Modes\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 4: Parquet INT96 & DateTime Rebase Modes (LEGACY)\")\n",
    "print(\"=\"*70)\n",
    "# Test Parquet write/read with LEGACY rebasing using simple modern date\n",
    "temp_dir = f\"{location}/functional_tests/parquet_legacy\"\n",
    "try:\n",
    "    # Create test data using SQL (faster than createDataFrame)\n",
    "    df_timestamps = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            1 as id,\n",
    "            timestamp('2020-01-01 00:00:00') as event_time\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            2 as id,\n",
    "            timestamp('1950-06-15 12:30:00') as event_time\n",
    "    \"\"\")\n",
    "    \n",
    "    # Write to Parquet with LEGACY rebasing\n",
    "    df_timestamps.write.mode(\"overwrite\").parquet(temp_dir)\n",
    "    print(f\"✓ Written Parquet with LEGACY rebasing\")\n",
    "    \n",
    "    # Read back with LEGACY rebasing\n",
    "    df_read = spark.read.parquet(temp_dir)\n",
    "    count = df_read.count()\n",
    "    print(f\"✓ Read Parquet with LEGACY rebasing: {count} rows\\n\")\n",
    "    \n",
    "    # Display the data\n",
    "    df_read.show(truncate=False)\n",
    "    \n",
    "    # Verify configurations\n",
    "    print(f\"\\nConfiguration values:\")\n",
    "    print(f\"  int96RebaseModeInRead: {spark.conf.get('spark.sql.parquet.int96RebaseModeInRead')}\")\n",
    "    print(f\"  int96RebaseModeInWrite: {spark.conf.get('spark.sql.parquet.int96RebaseModeInWrite')}\")\n",
    "    print(f\"  datetimeRebaseModeInRead: {spark.conf.get('spark.sql.parquet.datetimeRebaseModeInRead')}\")\n",
    "    print(f\"  datetimeRebaseModeInWrite: {spark.conf.get('spark.sql.parquet.datetimeRebaseModeInWrite')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Parquet test: {str(e)[:200]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06a7341c-287d-4e0f-a83f-622ca2c0e266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 5: spark.databricks.safespark.externalUDF.plan.limit (25)\n",
      "======================================================================\n",
      "✓ SafeSpark UDF limit: 25\n",
      "  This limit restricts the number of external UDFs in a query plan\n",
      "\n",
      "Testing with 26 UDFs (exceeds limit of 25)...\n",
      "✓ Created 26 UDFs\n",
      "✓ Query executed with 26 UDFs: 1 rows\n",
      "  (SafeSpark may have applied optimizations or restrictions)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: TEST 5 - SafeSpark External UDF Plan Limit\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 5: spark.databricks.safespark.externalUDF.plan.limit (25)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ SafeSpark UDF limit: {spark.conf.get('spark.databricks.safespark.externalUDF.plan.limit')}\")\n",
    "print(\"  This limit restricts the number of external UDFs in a single query plan\")\n",
    "print(\"\\nTesting with 26 UDFs (exceeds limit of 25)...\\n\")\n",
    "\n",
    "try:\n",
    "    # Create 26 UDFs (just over the limit of 25)\n",
    "    udfs = []\n",
    "    for i in range(26):\n",
    "        udf_func = udf(lambda x, i=i: f\"udf_{i}_{x}\" if x else None, StringType())\n",
    "        udfs.append(udf_func)\n",
    "    \n",
    "    print(f\"✓ Created {len(udfs)} UDFs\")\n",
    "    \n",
    "    # Create test DataFrame\n",
    "    df_test = spark.sql(\"SELECT 'test' as value\")\n",
    "    \n",
    "    # Apply all 26 UDFs in a SINGLE select() call\n",
    "    # This creates a single query plan with all 26 UDFs, which should hit the limit\n",
    "    print(f\"\\nAttempting to use all {len(udfs)} UDFs in a single select() statement...\")\n",
    "    \n",
    "    # Build select expression with all UDFs at once\n",
    "    select_exprs = [\"value\"] + [udfs[i](\"value\").alias(f\"udf_{i}\") for i in range(26)]\n",
    "    df_result = df_test.select(*select_exprs)\n",
    "    \n",
    "    # Try to execute the query - this should fail or be restricted\n",
    "    result_count = df_result.count()\n",
    "    print(f\"⚠️  Query unexpectedly succeeded with {len(udfs)} UDFs: {result_count} rows\")\n",
    "    print(\"  (Limit may not be enforced, or optimization bypassed it)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✓ Expected behavior - SafeSpark limit enforced!\")\n",
    "    print(f\"   Error: {str(e)[:300]}\")\n",
    "    print(f\"\\n   This demonstrates the UDF limit prevents queries with >{spark.conf.get('spark.databricks.safespark.externalUDF.plan.limit')} UDFs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fea6a09-0efd-4f4e-ad8f-f71673eaf1ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 6: spark.network.timeout (800)\n",
      "======================================================================\n",
      "Network timeout: 800 seconds\n",
      "✓ Timeout set to prevent network failures on long operations\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 9: TEST 6 - Network Timeout\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 6: spark.network.timeout (800)\")\n",
    "print(\"=\"*70)\n",
    "timeout_value = spark.conf.get(\"spark.network.timeout\")\n",
    "print(f\"Network timeout: {timeout_value} seconds\")\n",
    "print(f\"✓ Timeout set to prevent network failures on long operations\")\n",
    "# Note: Full timeout testing requires actual network delays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91d2207-265a-476b-9db9-aaafe65852c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEST 7: Other Configurations\n",
      "======================================================================\n",
      "spark.databricks.delta.preview.enabled: true\n",
      "spark.databricks.dataLineage.enabled: true\n",
      "spark.memory.offHeap.enabled: false\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Xss32M\n",
      "\n",
      "======================================================================\n",
      "✓ ALL FUNCTIONAL TESTS COMPLETED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: TEST 7 - Other Spark Configurations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 7: Other Configurations\")\n",
    "print(\"=\"*70)\n",
    "print(f\"spark.databricks.delta.preview.enabled: {spark.conf.get('spark.databricks.delta.preview.enabled')}\")\n",
    "print(f\"spark.databricks.dataLineage.enabled: {spark.conf.get('spark.databricks.dataLineage.enabled')}\")\n",
    "print(f\"spark.memory.offHeap.enabled: {spark.conf.get('spark.memory.offHeap.enabled')}\")\n",
    "print(f\"spark.driver.extraJavaOptions: {spark.conf.get('spark.driver.extraJavaOptions')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ ALL FUNCTIONAL TESTS COMPLETED\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12dc163-2665-4315-975a-3383bde676d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demonstrating html2text Library from init script...\n",
      "✓ html2text library imported successfully\n",
      "\n",
      "Converted HTML to plain text:\n",
      "Hello, world!\n",
      "\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Demonstrate html2text Library - Parse HTML Content\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nDemonstrating html2text Library from init script...\")\n",
    "\n",
    "try:\n",
    "    import html2text\n",
    "    \n",
    "    print(\"✓ html2text library imported successfully\")\n",
    "    \n",
    "    # Simple example of html2text usage\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = True\n",
    "    plain_text = h.handle(\"<p>Hello, <a href='https://www.google.com/earth/'>world</a>!</p>\")\n",
    "    print(\"\\nConverted HTML to plain text:\")\n",
    "    print(plain_text)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"✗ html2text not available: {e}\")\n",
    "    print(\"  Note: html2text is installed via init script on classic compute\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0800b6-bc01-48fb-8403-a7ed5eb33743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FUNCTIONAL TESTING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "✓ Job Configuration:\n",
      "    - Catalog (Spark config): maggiedatabricksterraform_dbw\n",
      "    - Database (env var): prod\n",
      "    - Location (env var): /Volumes/maggiedatabricksterraform_dbw/synthea/functional_testing\n",
      "\n",
      "✓ Spark Configuration Tests Completed:\n",
      "    - TEST 1: spark.databricks.sql.initial.catalog.name - Verified default catalog\n",
      "    - TEST 2: spark.sql.storeAssignmentPolicy (LEGACY) - Tested type coercion\n",
      "    - TEST 3: spark.sql.legacy.timeParserPolicy (LEGACY) - Tested date parsing\n",
      "    - TEST 4: Parquet INT96 & DateTime rebasing - Tested read/write with legacy timestamps\n",
      "    - TEST 5: spark.databricks.safespark.externalUDF.plan.limit - Tested multiple UDFs\n",
      "    - TEST 6: spark.network.timeout - Verified timeout configuration\n",
      "    - TEST 7: Other configs - Delta preview, data lineage, memory, JVM options\n",
      "\n",
      "✓ Custom Library Tests:\n",
      "    - html2text: Tested HTML to plain text conversion\n",
      "\n",
      "✓ All functional tests completed successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Summary and Completion\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FUNCTIONAL TESTING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ Job Configuration:\")\n",
    "print(f\"    - Catalog (Spark config): {catalog_name}\")\n",
    "print(f\"    - Database (env var): {database_name}\")\n",
    "print(f\"    - Location (env var): {location}\")\n",
    "print(\"\\n✓ Spark Configuration Tests Completed:\")\n",
    "print(\"    - TEST 1: spark.databricks.sql.initial.catalog.name - Verified default catalog\")\n",
    "print(\"    - TEST 2: spark.sql.storeAssignmentPolicy (LEGACY) - Tested type coercion\")\n",
    "print(\"    - TEST 3: spark.sql.legacy.timeParserPolicy (LEGACY) - Tested date parsing\")\n",
    "print(\"    - TEST 4: Parquet INT96 & DateTime rebasing - Tested read/write with legacy timestamps\")\n",
    "print(\"    - TEST 5: spark.databricks.safespark.externalUDF.plan.limit - Tested multiple UDFs\")\n",
    "print(\"    - TEST 6: spark.network.timeout - Verified timeout configuration\")\n",
    "print(\"    - TEST 7: Other configs - Delta preview, data lineage, memory, JVM options\")\n",
    "print(\"\\n✓ Custom Library Tests:\")\n",
    "print(\"    - html2text: Tested HTML to plain text conversion\")\n",
    "print(\"\\n✓ All functional tests completed successfully!\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functional_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
