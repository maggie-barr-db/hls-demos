{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Data Quality Profiling Notebook\n",
        "# =============================================================================\n",
        "# This notebook profiles silver and gold tables for data quality metrics:\n",
        "# - Row counts\n",
        "# - Null counts per column\n",
        "# - Distinct value counts\n",
        "# - Min/Max values for numeric columns\n",
        "# - Data completeness percentages\n",
        "#\n",
        "# Results are stored in: {catalog}.data_quality.table_profiling\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "\n",
        "def profile_table(spark, catalog_name, schema_name, table_name):\n",
        "    \"\"\"Profile a table and return quality metrics.\"\"\"\n",
        "    \n",
        "    full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
        "    print(f\"\\nProfiling: {full_table_name}\")\n",
        "    \n",
        "    try:\n",
        "        df = spark.table(full_table_name)\n",
        "        \n",
        "        # Get row count\n",
        "        row_count = df.count()\n",
        "        print(f\"  Rows: {row_count:,}\")\n",
        "        \n",
        "        # Get column count\n",
        "        column_count = len(df.columns)\n",
        "        print(f\"  Columns: {column_count}\")\n",
        "        \n",
        "        metrics = []\n",
        "        \n",
        "        for col_name in df.columns:\n",
        "            col_type = dict(df.dtypes)[col_name]\n",
        "            \n",
        "            # Calculate null count and null percentage\n",
        "            null_count = df.filter(F.col(col_name).isNull()).count()\n",
        "            null_pct = (null_count / row_count * 100) if row_count > 0 else 0\n",
        "            \n",
        "            # Calculate distinct count\n",
        "            distinct_count = df.select(col_name).distinct().count()\n",
        "            \n",
        "            # Get min/max for numeric columns\n",
        "            min_val = None\n",
        "            max_val = None\n",
        "            if col_type in ['int', 'bigint', 'double', 'float', 'decimal']:\n",
        "                stats = df.agg(\n",
        "                    F.min(col_name).alias('min_val'),\n",
        "                    F.max(col_name).alias('max_val')\n",
        "                ).collect()[0]\n",
        "                min_val = str(stats['min_val']) if stats['min_val'] is not None else None\n",
        "                max_val = str(stats['max_val']) if stats['max_val'] is not None else None\n",
        "            \n",
        "            metrics.append({\n",
        "                'catalog_name': catalog_name,\n",
        "                'schema_name': schema_name,\n",
        "                'table_name': table_name,\n",
        "                'column_name': col_name,\n",
        "                'data_type': col_type,\n",
        "                'total_rows': row_count,\n",
        "                'null_count': null_count,\n",
        "                'null_percentage': round(null_pct, 2),\n",
        "                'distinct_count': distinct_count,\n",
        "                'min_value': min_val,\n",
        "                'max_value': max_val,\n",
        "                'profiling_timestamp': datetime.utcnow(),\n",
        "                'profiling_run_id': datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "            })\n",
        "        \n",
        "        print(f\"  ✓ Profiled {len(metrics)} columns\")\n",
        "        return metrics\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ✗ Error profiling table: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "    \n",
        "    # Get parameters from Databricks widgets\n",
        "    try:\n",
        "        from pyspark.dbutils import DBUtils\n",
        "        dbutils = DBUtils(spark)\n",
        "        catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "    except Exception:\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument(\"--catalog_name\", type=str, required=True)\n",
        "        args, _ = parser.parse_known_args()\n",
        "        catalog_name = args.catalog_name\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"DATA QUALITY PROFILING\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Catalog: {catalog_name}\")\n",
        "    print(f\"Target Schema: data_quality\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Define tables to profile\n",
        "    tables_to_profile = [\n",
        "        # Silver tables\n",
        "        ('synthea', 'claims_silver'),\n",
        "        ('synthea', 'medications_silver'),\n",
        "        ('synthea', 'patient_encounters_silver'),\n",
        "        ('synthea', 'procedures_silver'),\n",
        "        # Gold tables\n",
        "        ('synthea', 'member_monthly_snapshot_gold'),\n",
        "    ]\n",
        "    \n",
        "    # Collect all metrics\n",
        "    all_metrics = []\n",
        "    \n",
        "    for schema_name, table_name in tables_to_profile:\n",
        "        metrics = profile_table(spark, catalog_name, schema_name, table_name)\n",
        "        all_metrics.extend(metrics)\n",
        "    \n",
        "    if not all_metrics:\n",
        "        print(\"\\n⚠️  No metrics collected\")\n",
        "        return\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    metrics_df = spark.createDataFrame(all_metrics)\n",
        "    \n",
        "    # Create data_quality schema if it doesn't exist\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.data_quality\")\n",
        "    print(f\"\\n✓ Schema created/verified: {catalog_name}.data_quality\")\n",
        "    \n",
        "    # Append to data quality table\n",
        "    target_table = f\"{catalog_name}.data_quality.table_profiling\"\n",
        "    \n",
        "    metrics_df.write \\\n",
        "        .mode(\"append\") \\\n",
        "        .format(\"delta\") \\\n",
        "        .option(\"mergeSchema\", \"true\") \\\n",
        "        .saveAsTable(target_table)\n",
        "    \n",
        "    total_metrics = len(all_metrics)\n",
        "    total_tables = len(tables_to_profile)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"✓ Data quality profiling complete!\")\n",
        "    print(f\"  Tables profiled: {total_tables}\")\n",
        "    print(f\"  Metrics collected: {total_metrics:,}\")\n",
        "    print(f\"  Results stored in: {target_table}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Show summary\n",
        "    print(\"Summary by table:\")\n",
        "    summary = metrics_df.groupBy('schema_name', 'table_name') \\\n",
        "        .agg(\n",
        "            F.first('total_rows').alias('row_count'),\n",
        "            F.count('column_name').alias('column_count'),\n",
        "            F.avg('null_percentage').alias('avg_null_pct')\n",
        "        ) \\\n",
        "        .orderBy('schema_name', 'table_name')\n",
        "    \n",
        "    summary.show(truncate=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
