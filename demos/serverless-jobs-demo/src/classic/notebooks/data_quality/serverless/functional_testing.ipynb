{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "72bfddb9-82d6-490a-b4df-90e1ccfad798",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "FUNCTIONAL TESTING - Library & Configuration Demonstration\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Functional Testing Notebook\n",
        "# =============================================================================\n",
        "# This notebook demonstrates:\n",
        "# - Custom library usage (html2text)\n",
        "# - Job parameter configuration to replace default catalog config, cluster environment variable\n",
        "# - Spark configuration settings\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import argparse\n",
        "from pyspark.dbutils import DBUtils\n",
        "\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType, IntegerType, TimestampType, StructType, StructField\n",
        "from datetime import datetime\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "\n",
        "# Get Spark session\n",
        "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Initialize test results tracker\n",
        "test_results = {\n",
        "    \"TEST 1\": {\"name\": \"Catalog config (job parameter workaround)\", \"status\": \"PENDING\"},\n",
        "    \"TEST 2\": {\"name\": \"Type conversion (ANSI mode workaround)\", \"status\": \"PENDING\"},\n",
        "    \"TEST 3\": {\"name\": \"Time parser (SUPPORTED)\", \"status\": \"PENDING\"},\n",
        "    \"TEST 4\": {\"name\": \"Parquet timestamps (code refactoring required)\", \"status\": \"PENDING\"},\n",
        "    \"TEST 5\": {\"name\": \"UDF limits (managed by Databricks)\", \"status\": \"PENDING\"},\n",
        "    \"TEST 6\": {\"name\": \"Execution timeout (workaround)\", \"status\": \"PENDING\"},\n",
        "    \"TEST 7\": {\"name\": \"Default configurations\", \"status\": \"PENDING\"},\n",
        "    \"LIBRARY\": {\"name\": \"html2text library\", \"status\": \"PENDING\"}\n",
        "}\n",
        "\n",
        "dbutils = DBUtils(spark)\n",
        "\n",
        "# Get parameters from job configuration (base_parameters in job task definition)\n",
        "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
        "database_name = dbutils.widgets.get(\"database_name\")\n",
        "location = dbutils.widgets.get(\"location\")\n",
        "\n",
        "print(\"\\nJob Configuration (from job task parameters):\")\n",
        "print(f\"  Catalog Name: {catalog_name}\")\n",
        "print(f\"  Database Name: {database_name}\")\n",
        "print(f\"  Location: {location}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fabdf4d0-a9d3-4507-ae78-121ff0f80d61",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 1: spark.databricks.sql.initial.catalog.name (synthea)\n",
            "======================================================================\n",
            "Current catalog: maggiedatabricksterraform_dbw\n",
            "‚úì Can query without catalog prefix, initial catalog is: maggiedatabricksterraform_dbw\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 1 - Catalog Configuration (Job Parameter Workaround)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 1: Catalog Configuration\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  Classic Config: spark.databricks.sql.initial.catalog.name\")\n",
        "print(\"‚úì Serverless Workaround: Use catalog_name from job parameter + USE CATALOG\\n\")\n",
        "\n",
        "print(f\"Catalog from job parameter: {catalog_name}\")\n",
        "current_catalog = spark.catalog.currentCatalog()\n",
        "print(f\"Current active catalog: {current_catalog}\")\n",
        "\n",
        "# Set the catalog using USE CATALOG statement\n",
        "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
        "print(f\"\\n‚úì Set default catalog to: {catalog_name}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# In serverless, we use the catalog_name from job parameters\n",
        "# instead of spark.databricks.sql.initial.catalog.name\n",
        "\n",
        "\n",
        "# Verify we can query without catalog prefix and show schemas/databases\n",
        "try:\n",
        "    test_query = \"SHOW DATABASES\"\n",
        "    result = spark.sql(test_query)\n",
        "    print(f\"‚úì Can query without catalog prefix in catalog: {catalog_name}\")\n",
        "    print(f\"\\nSchemas/Databases in catalog '{catalog_name}':\")\n",
        "    result.show(truncate=False)\n",
        "    test_results[\"TEST 1\"][\"status\"] = \"PASSED\"\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Query error: {str(e)[:100]}\")\n",
        "    test_results[\"TEST 1\"][\"status\"] = \"FAILED\"\n",
        "    test_results[\"TEST 1\"][\"error\"] = str(e)[:100]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "47ef9fc1-cbc2-43f3-ac40-15b4beaf0712",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 2: spark.sql.storeAssignmentPolicy (LEGACY)\n",
            "======================================================================\n",
            "Testing implicit type conversions with LEGACY policy...\n",
            "\n",
            "‚úì LEGACY policy type coercion results:\n",
            "+---------------+-------------+\n",
            "|original_string|casted_to_int|\n",
            "+---------------+-------------+\n",
            "|123            |123          |\n",
            "|456            |456          |\n",
            "|not_a_number   |NULL         |\n",
            "|789            |789          |\n",
            "|invalid        |NULL         |\n",
            "+---------------+-------------+\n",
            "\n",
            "\n",
            "Observation:\n",
            "  - Valid strings ('123', '456', '789') converted to integers\n",
            "  - Invalid strings ('not_a_number', 'invalid') converted to NULL\n",
            "  - No errors thrown - this is the LEGACY behavior\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 2 - Type Conversion (ANSI Mode Workaround)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 2: Lenient Type Conversion\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  Classic Config: spark.sql.storeAssignmentPolicy = LEGACY\")\n",
        "print(\"‚úì Serverless Workaround: spark.sql.ansi.enabled = False\\n\")\n",
        "\n",
        "# WORKAROUND: Use spark.sql.ansi.enabled instead of spark.sql.storeAssignmentPolicy\n",
        "# Setting to false provides lenient type conversions (invalid values become NULL)\n",
        "# ‚ö†Ô∏è  WARNING: This has broader implications than just storeAssignmentPolicy:\n",
        "#     - Division by zero returns NULL instead of error\n",
        "#     - Arithmetic overflow wraps around or returns NULL\n",
        "#     - More permissive type coercion and function behavior\n",
        "\n",
        "spark.conf.set(\"spark.sql.ansi.enabled\", False)\n",
        "\n",
        "# Verify the config is set\n",
        "ansi_enabled = spark.conf.get(\"spark.sql.ansi.enabled\")\n",
        "print(f\"Current spark.sql.ansi.enabled: {ansi_enabled}\")\n",
        "print(\"(False provides lenient type conversions like LEGACY policy)\\n\")\n",
        "\n",
        "try:\n",
        "    # Create DataFrame with string values\n",
        "    df_test = spark.createDataFrame([\n",
        "        (\"123\",),\n",
        "        (\"456\",),\n",
        "        (\"not_a_number\",),\n",
        "        (\"789\",),\n",
        "        (\"invalid\",)\n",
        "    ], [\"value_str\"])\n",
        "    \n",
        "    # Apply CAST - with ANSI disabled, invalid values convert to NULL without errors\n",
        "    # This provides the same behavior as storeAssignmentPolicy LEGACY\n",
        "    df_result = df_test.selectExpr(\n",
        "        \"value_str as original_string\",\n",
        "        \"CAST(value_str AS INT) as casted_to_int\"\n",
        "    )\n",
        "    \n",
        "    print(\"‚úì Type coercion results with ANSI disabled:\")\n",
        "    df_result.show(truncate=False)\n",
        "    \n",
        "    print(\"\\nObservation:\")\n",
        "    print(\"  - Valid strings ('123', '456', '789') converted to integers\")\n",
        "    print(\"  - Invalid strings ('not_a_number', 'invalid') converted to NULL\")\n",
        "    print(\"  - No errors thrown - spark.sql.ansi.enabled=False provides LEGACY-like behavior\")\n",
        "    \n",
        "    print(\"\\n‚ö†Ô∏è  IMPORTANT: spark.sql.ansi.enabled=False has BROADER implications:\")\n",
        "    print(\"  - Division by zero: Returns NULL instead of error\")\n",
        "    print(\"  - Arithmetic overflow: Wraps around or returns NULL instead of error\")\n",
        "    print(\"  - Type coercion: More permissive implicit conversions\")\n",
        "    print(\"  - Function behavior: More lenient error handling (e.g., to_date with invalid dates)\")\n",
        "    print(\"  - SQL parsing: Reserved keywords can be used as identifiers\")\n",
        "    print(\"\\n  This is NOT a perfect 1:1 replacement for storeAssignmentPolicy LEGACY.\")\n",
        "    print(\"  Review your code for operations that may be affected by these changes.\")\n",
        "    \n",
        "    test_results[\"TEST 2\"][\"status\"] = \"PASSED\"\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error with type conversion: {str(e)[:200]}\")\n",
        "    print(\"  Note: If this fails, spark.sql.ansi.enabled may not be set correctly\")\n",
        "    test_results[\"TEST 2\"][\"status\"] = \"FAILED\"\n",
        "    test_results[\"TEST 2\"][\"error\"] = str(e)[:200]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2dbafb61-bb22-4426-8ce9-7da5d0481090",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 3: spark.sql.legacy.timeParserPolicy (LEGACY)\n",
            "======================================================================\n",
            "‚úì LEGACY time parser results:\n",
            "  to_date('2024-01-15') = 2024-01-15\n",
            "  to_timestamp('2024-01-15 14:30:00') = 2024-01-15 14:30:00\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 3 - Legacy Time Parser Policy (SUPPORTED)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 3: Legacy Time Parser Policy\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚úÖ SUPPORTED in Serverless: spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
        "print(\"Testing lenient date/time parsing with LEGACY policy...\\n\")\n",
        "\n",
        "# SUPPORTED: Legacy Time Parser Policy - enables lenient date/time parsing\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "# Verify the config is set\n",
        "current_policy = spark.conf.get(\"spark.sql.legacy.timeParserPolicy\")\n",
        "print(f\"Current timeParserPolicy: {current_policy}\\n\")\n",
        "\n",
        "try:\n",
        "    # LEGACY policy is more lenient with date formats and edge cases\n",
        "    # Create DataFrame with various date/time formats\n",
        "    df_dates = spark.createDataFrame([\n",
        "        (\"2024-01-15\",),           # Standard ISO format\n",
        "        (\"2024-01-15 14:30:00\",),  # With time\n",
        "        (\"15-01-2024\",),           # Day-first format (may not parse)\n",
        "        (\"01/15/2024\",),           # Slash separator (may not parse)\n",
        "        (\"2024-1-5\",),             # Single digit month/day\n",
        "        (\"invalid_date\",)          # Invalid format\n",
        "    ], [\"date_string\"])\n",
        "    \n",
        "    # Try parsing with LEGACY policy - it's more forgiving than CORRECTED or EXCEPTION\n",
        "    df_parsed = df_dates.selectExpr(\n",
        "        \"date_string\",\n",
        "        \"to_date(date_string) as parsed_date\",\n",
        "        \"to_timestamp(date_string) as parsed_timestamp\"\n",
        "    )\n",
        "    \n",
        "    print(\"‚úì LEGACY time parser results:\")\n",
        "    df_parsed.show(truncate=False)\n",
        "    \n",
        "    print(\"\\nObservation:\")\n",
        "    print(\"  - LEGACY policy attempts to parse various date formats\")\n",
        "    print(\"  - Invalid or unsupported formats result in NULL rather than throwing errors\")\n",
        "    print(\"  - More lenient than CORRECTED or EXCEPTION policies\")\n",
        "    print(\"  - Handles edge cases with single-digit months/days gracefully\")\n",
        "    test_results[\"TEST 3\"][\"status\"] = \"PASSED\"\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error with LEGACY time parser: {str(e)[:200]}\")\n",
        "    print(\"  Note: If this fails, the spark.sql.legacy.timeParserPolicy may not be set correctly\")\n",
        "    test_results[\"TEST 3\"][\"status\"] = \"FAILED\"\n",
        "    test_results[\"TEST 3\"][\"error\"] = str(e)[:200]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a84d96b9-14e4-4a5d-ac93-306247cffce8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 4: Parquet INT96 & DateTime Rebase Modes (LEGACY)\n",
            "======================================================================\n",
            "‚úì Written Parquet with LEGACY rebasing\n",
            "‚úì Read Parquet with LEGACY rebasing: 2 rows\n",
            "  int96RebaseModeInRead: LEGACY\n",
            "  int96RebaseModeInWrite: LEGACY\n",
            "  datetimeRebaseModeInRead: LEGACY\n",
            "  datetimeRebaseModeInWrite: LEGACY\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 4 - Parquet Timestamp Handling (Code Refactoring Required)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 4: Parquet Timestamp Handling\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  Classic Configs: spark.sql.parquet.int96RebaseModeInRead/Write = LEGACY\")\n",
        "print(\"‚ö†Ô∏è  Classic Configs: spark.sql.parquet.datetimeRebaseModeInRead/Write = LEGACY\")\n",
        "print(\"‚úì Serverless Workaround: Code refactoring required\\n\")\n",
        "\n",
        "print(\"Note: Parquet rebase mode configs are NOT SUPPORTED in serverless.\")\n",
        "print(\"For legacy Parquet files with timestamps, consider:\")\n",
        "print(\"  1. Rewriting Parquet files with modern timestamp encoding\")\n",
        "print(\"  2. Using explicit timestamp conversion functions in code\")\n",
        "print(\"  3. Handling edge case dates (pre-1900, post-2262) in application logic\\n\")\n",
        "\n",
        "# Test Parquet write/read with LEGACY rebasing\n",
        "temp_dir = f\"{location}/functional_tests/parquet_legacy\"\n",
        "try:\n",
        "    # Create test data with various timestamps including edge cases\n",
        "    df_timestamps = spark.sql(\"\"\"\n",
        "        SELECT \n",
        "            1 as id,\n",
        "            timestamp('2020-01-01 00:00:00') as event_time\n",
        "        UNION ALL\n",
        "        SELECT \n",
        "            2 as id,\n",
        "            timestamp('1950-06-15 12:30:00') as event_time\n",
        "    \"\"\")\n",
        "    \n",
        "    print(\"Test data created (modern timestamps only):\")\n",
        "    df_timestamps.show(truncate=False)\n",
        "    \n",
        "    # Write to Parquet (serverless uses modern timestamp encoding by default)\n",
        "    df_timestamps.write.mode(\"overwrite\").parquet(temp_dir)\n",
        "    print(f\"‚úì Written Parquet to: {temp_dir}\")\n",
        "    \n",
        "    # Read back\n",
        "    df_read = spark.read.parquet(temp_dir)\n",
        "    count = df_read.count()\n",
        "    print(f\"‚úì Read Parquet: {count} rows\")\n",
        "    \n",
        "    print(\"\\nVerifying data integrity:\")\n",
        "    df_read.show(truncate=False)\n",
        "    \n",
        "    print(\"\\nObservation:\")\n",
        "    print(\"  - Serverless uses modern Parquet timestamp encoding by default\")\n",
        "    print(\"  - For legacy Parquet files with INT96 timestamps, code refactoring is needed\")\n",
        "    print(\"  - Avoid dates before 1900 or after 2262 in new data\")\n",
        "    test_results[\"TEST 4\"][\"status\"] = \"PASSED\"\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Parquet test error: {str(e)[:200]}\")\n",
        "    print(\"  Note: This test demonstrates modern Parquet handling in serverless\")\n",
        "    test_results[\"TEST 4\"][\"status\"] = \"FAILED\"\n",
        "    test_results[\"TEST 4\"][\"error\"] = str(e)[:200]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "06a7341c-287d-4e0f-a83f-622ca2c0e266",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 5: spark.databricks.safespark.externalUDF.plan.limit (25)\n",
            "======================================================================\n",
            "‚úì SafeSpark UDF limit: 25\n",
            "  This limit restricts the number of external UDFs in a query plan\n",
            "\n",
            "Testing with 26 UDFs (exceeds limit of 25)...\n",
            "‚úì Created 26 UDFs\n",
            "‚úì Query executed with 26 UDFs: 1 rows\n",
            "  (SafeSpark may have applied optimizations or restrictions)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 5 - UDF Limits in Serverless\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 5: UDF Behavior (Managed by Databricks)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  Classic Config: spark.databricks.safespark.externalUDF.plan.limit = 25\")\n",
        "print(\"‚úì Serverless: UDF limits managed by Databricks (not user-configurable)\\n\")\n",
        "\n",
        "print(\"Note on UDFs in Serverless:\")\n",
        "print(\"  - In classic compute, spark.databricks.safespark.externalUDF.plan.limit\")\n",
        "print(\"    restricts the number of external UDFs in a single query plan\")\n",
        "print(\"  - In serverless compute, UDF behavior is managed by Databricks at the platform level\")\n",
        "print(\"  - There is NO user-configurable UDF limit setting in serverless\")\n",
        "print(\"  - This test verifies UDFs work, but will not test any limits\")\n",
        "print(\"  - Best practice: Minimize UDF usage and use built-in Spark functions instead\")\n",
        "\n",
        "print(\"\\nVerifying UDF functionality in serverless...\\n\")\n",
        "\n",
        "try:\n",
        "    # Create 26 UDFs (exceeds classic limit of 25, but serverless has no user-configurable limit)\n",
        "    udfs = []\n",
        "    for i in range(26):\n",
        "        udf_func = udf(lambda x, i=i: f\"udf_{i}_{x}\" if x else None, StringType())\n",
        "        udfs.append(udf_func)\n",
        "    \n",
        "    print(f\"‚úì Created {len(udfs)} UDFs (exceeds classic limit of 25)\")\n",
        "    \n",
        "    # Create test DataFrame\n",
        "    df_test = spark.sql(\"SELECT 'test' as value\")\n",
        "    \n",
        "    # Apply UDFs in a single select() call\n",
        "    print(f\"\\nApplying all {len(udfs)} UDFs in a single select() statement...\")\n",
        "    \n",
        "    # Build select expression with all UDFs at once\n",
        "    select_exprs = [\"value\"] + [udfs[i](\"value\").alias(f\"udf_{i}\") for i in range(len(udfs))]\n",
        "    df_result = df_test.select(*select_exprs)\n",
        "    \n",
        "    # Execute the query\n",
        "    result_count = df_result.count()\n",
        "    print(f\"‚úì Query executed successfully with {len(udfs)} UDFs: {result_count} rows\")\n",
        "    \n",
        "    print(\"\\nObservation:\")\n",
        "    print(\"  - Serverless successfully handles 26 UDFs (exceeds classic limit of 25)\")\n",
        "    print(\"  - No user-configurable UDF limit in serverless\")\n",
        "    print(\"  - UDF behavior is managed by Databricks platform\")\n",
        "    print(\"  - Best practice: Minimize UDF usage and prefer built-in Spark functions\")\n",
        "    test_results[\"TEST 5\"][\"status\"] = \"PASSED\"\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è UDF test error: {str(e)[:300]}\")\n",
        "    print(\"  Note: UDF limits are managed internally by Databricks in serverless\")\n",
        "    test_results[\"TEST 5\"][\"status\"] = \"FAILED\"\n",
        "    test_results[\"TEST 5\"][\"error\"] = str(e)[:300]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0fea6a09-0efd-4f4e-ad8f-f71673eaf1ac",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 6: spark.network.timeout (800)\n",
            "======================================================================\n",
            "Network timeout: 800 seconds\n",
            "‚úì Timeout set to prevent network failures on long operations\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 6 - Execution Timeout (Serverless)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 6: Execution Timeout (Workaround)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚ö†Ô∏è  Classic Config: spark.network.timeout = 800\")\n",
        "print(\"‚úì Serverless Workaround: spark.databricks.execution.timeout = 800s\\n\")\n",
        "\n",
        "print(\"\\nNote on timeout configurations:\")\n",
        "print(\"  - spark.network.timeout is NOT SUPPORTED in serverless\")\n",
        "print(\"  - Use spark.databricks.execution.timeout instead\")\n",
        "print(\"  - Controls the maximum execution time for queries/operations\")\n",
        "print(\"  - Helps prevent long-running or hung operations\")\n",
        "\n",
        "# WORKAROUND: Execution timeout for serverless (replaces spark.network.timeout)\n",
        "spark.conf.set(\"spark.databricks.execution.timeout\", \"800s\")\n",
        "print(\"‚úì Set spark.databricks.execution.timeout = 800s (replaces network.timeout)\")\n",
        "\n",
        "# Verify the config is set\n",
        "timeout_value = spark.conf.get(\"spark.databricks.execution.timeout\")\n",
        "print(f\"‚úì Execution timeout: {timeout_value}\")\n",
        "\n",
        "test_results[\"TEST 6\"][\"status\"] = \"PASSED\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d91d2207-265a-476b-9db9-aaafe65852c0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "TEST 7: Other Configurations\n",
            "======================================================================\n",
            "spark.databricks.delta.preview.enabled: true\n",
            "spark.databricks.dataLineage.enabled: true\n",
            "spark.memory.offHeap.enabled: false\n",
            "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Xss32M\n",
            "\n",
            "======================================================================\n",
            "‚úì ALL FUNCTIONAL TESTS COMPLETED\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# TEST 7 - Verify All Configurations\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST 7: Default Configurations (Enabled by Default)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n‚úÖ The following are enabled by default in serverless:\\n\")\n",
        "\n",
        "# Check default configurations\n",
        "try:\n",
        "    delta_preview = spark.conf.get('spark.databricks.delta.preview.enabled')\n",
        "    print(f\"  - spark.databricks.delta.preview.enabled: {delta_preview}\")\n",
        "except:\n",
        "    print(f\"  - spark.databricks.delta.preview.enabled: true (default)\")\n",
        "\n",
        "try:\n",
        "    data_lineage = spark.conf.get('spark.databricks.dataLineage.enabled')\n",
        "    print(f\"  - spark.databricks.dataLineage.enabled: {data_lineage} (in Unity Catalog)\")\n",
        "except:\n",
        "    print(f\"  - spark.databricks.dataLineage.enabled: true (default in Unity Catalog)\")\n",
        "\n",
        "try:\n",
        "    offheap = spark.conf.get('spark.memory.offHeap.enabled')\n",
        "    print(f\"  - spark.memory.offHeap.enabled: {offheap}\")\n",
        "except:\n",
        "    print(f\"  - spark.memory.offHeap.enabled: false (default)\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  NOT SUPPORTED in serverless (managed by Databricks):\")\n",
        "print(\"  - spark.driver.extraJavaOptions (JVM options managed by platform)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úì ALL FUNCTIONAL TESTS COMPLETED FOR SERVERLESS\")\n",
        "print(\"=\"*70)\n",
        "test_results[\"TEST 7\"][\"status\"] = \"PASSED\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c12dc163-2665-4315-975a-3383bde676d7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Demonstrating html2text Library from init script...\n",
            "‚úì html2text library imported successfully\n",
            "\n",
            "Converted HTML to plain text:\n",
            "Hello, world!\n",
            "\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Demonstrate html2text Library - Parse HTML Content\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\nDemonstrating html2text Library from init script/wheel file...\")\n",
        "\n",
        "try:\n",
        "    import html2text\n",
        "    \n",
        "    print(\"‚úì html2text library imported successfully\")\n",
        "    \n",
        "    # Simple example of html2text usage\n",
        "    h = html2text.HTML2Text()\n",
        "    h.ignore_links = True\n",
        "    plain_text = h.handle(\"<p>Hello, <a href='https://www.google.com/earth/'>world</a>!</p>\")\n",
        "    print(\"\\nConverted HTML to plain text:\")\n",
        "    print(plain_text)\n",
        "    test_results[\"LIBRARY\"][\"status\"] = \"PASSED\"\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚úó html2text not available: {e}\")\n",
        "    print(\"  Note: html2text is installed via init script on classic compute\")\n",
        "    test_results[\"LIBRARY\"][\"status\"] = \"FAILED\"\n",
        "    test_results[\"LIBRARY\"][\"error\"] = str(e)\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ba0800b6-bc01-48fb-8403-a7ed5eb33743",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "FUNCTIONAL TESTING SUMMARY\n",
            "======================================================================\n",
            "\n",
            "‚úì Job Configuration:\n",
            "    - Catalog (Spark config): maggiedatabricksterraform_dbw\n",
            "    - Database (env var): prod\n",
            "    - Location (env var): /Volumes/maggiedatabricksterraform_dbw/synthea/functional_testing\n",
            "\n",
            "‚úì Spark Configuration Tests Completed:\n",
            "    - TEST 1: spark.databricks.sql.initial.catalog.name - Verified default catalog\n",
            "    - TEST 2: spark.sql.storeAssignmentPolicy (LEGACY) - Tested type coercion\n",
            "    - TEST 3: spark.sql.legacy.timeParserPolicy (LEGACY) - Tested date parsing\n",
            "    - TEST 4: Parquet INT96 & DateTime rebasing - Tested read/write with legacy timestamps\n",
            "    - TEST 5: spark.databricks.safespark.externalUDF.plan.limit - Tested multiple UDFs\n",
            "    - TEST 6: spark.network.timeout - Verified timeout configuration\n",
            "    - TEST 7: Other configs - Delta preview, data lineage, memory, JVM options\n",
            "\n",
            "‚úì Custom Library Tests:\n",
            "    - html2text: Tested HTML to plain text conversion\n",
            "\n",
            "‚úì All functional tests completed successfully!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Summary and Completion\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FUNCTIONAL TESTING SUMMARY - SERVERLESS COMPUTE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìã Job Configuration (from job task parameters):\")\n",
        "print(f\"    - Catalog: {catalog_name}\")\n",
        "print(f\"    - Database: {database_name}\")\n",
        "print(f\"    - Location: {location}\")\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  Serverless Spark Configurations Applied:\")\n",
        "print(\"    ‚úÖ SUPPORTED:\")\n",
        "print(\"       - spark.sql.legacy.timeParserPolicy = LEGACY\")\n",
        "print(\"    ‚úì WORKAROUNDS:\")\n",
        "print(\"       - spark.sql.ansi.enabled = False (replaces storeAssignmentPolicy)\")\n",
        "print(\"       - spark.databricks.execution.timeout = 800s (replaces network.timeout)\")\n",
        "print(\"       - catalog_name from job parameter (replaces initial.catalog.name)\")\n",
        "print(\"    ‚ö†Ô∏è  NOT SUPPORTED (require code refactoring):\")\n",
        "print(\"       - Parquet rebase modes (int96/datetime)\")\n",
        "print(\"       - SafeSpark UDF limits\")\n",
        "print(\"       - JVM options (extraJavaOptions)\")\n",
        "\n",
        "print(\"\\nüß™ Test Results:\")\n",
        "passed_count = 0\n",
        "failed_count = 0\n",
        "\n",
        "for test_key in [\"TEST 1\", \"TEST 2\", \"TEST 3\", \"TEST 4\", \"TEST 5\", \"TEST 6\", \"TEST 7\", \"LIBRARY\"]:\n",
        "    test = test_results[test_key]\n",
        "    status = test[\"status\"]\n",
        "    name = test[\"name\"]\n",
        "    \n",
        "    if status == \"PASSED\":\n",
        "        symbol = \"‚úÖ\"\n",
        "        passed_count += 1\n",
        "    elif status == \"FAILED\":\n",
        "        symbol = \"‚ùå\"\n",
        "        failed_count += 1\n",
        "    else:\n",
        "        symbol = \"‚è∏Ô∏è\"\n",
        "    \n",
        "    print(f\"    {symbol} {test_key}: {name} - {status}\")\n",
        "    if status == \"FAILED\" and \"error\" in test:\n",
        "        print(f\"        Error: {test['error']}\")\n",
        "\n",
        "print(f\"\\nüìä Summary: {passed_count} passed, {failed_count} failed out of {len(test_results)} tests\")\n",
        "\n",
        "if failed_count == 0:\n",
        "    print(\"\\n‚úÖ All functional tests completed successfully for serverless compute!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  {failed_count} test(s) failed. Please review the errors above.\")\n",
        "\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "environmentMetadata": {
        "base_environment": "",
        "dependencies": [
          "-r /Volumes/maggiedatabricksterraform_dbw/synthea/admin_configs/requirements_data_quality_serverless.txt"
        ],
        "environment_version": "4"
      }
    },
    "databricks": {
      "environment": {
        "environmentId": "data_quality_serverless_demo"
      }
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
