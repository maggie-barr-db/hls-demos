{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import argparse\n",
    "from silver_control import get_last_processed_run_id, update_last_processed_run_id, get_new_run_ids, get_all_run_ids\n",
    "\n",
    "\n",
    "def load_fact_medications_incremental(spark: SparkSession, catalog_name: str, run_ids: list[str]):\n",
    "    \"\"\"Load fact_medications table incrementally.\"\"\"\n",
    "    \n",
    "    run_ids_str = \"', '\".join(run_ids)\n",
    "    \n",
    "    df = spark.sql(f\"\"\"\n",
    "        SELECT\n",
    "            md5(concat(m.PATIENT, m.ENCOUNTER, cast(m.CODE as string), cast(m.START as string))) as medication_id,\n",
    "            m.PATIENT as patient_id,\n",
    "            m.ENCOUNTER as encounter_id,\n",
    "            m.PAYER as payer_id,\n",
    "            m.START as prescription_start_date,\n",
    "            m.STOP as prescription_stop_date,\n",
    "            DATE(m.START) as prescription_date_key,\n",
    "            datediff(m.STOP, m.START) as days_supply,\n",
    "            m.CODE as medication_code,\n",
    "            m.DESCRIPTION as medication_description,\n",
    "            m.DISPENSES as dispenses,\n",
    "            m.BASE_COST as base_cost,\n",
    "            m.PAYER_COVERAGE as payer_coverage,\n",
    "            m.BASE_COST - m.PAYER_COVERAGE as patient_copay,\n",
    "            m.TOTALCOST as total_cost,\n",
    "            m.REASONCODE as reason_code,\n",
    "            m.REASONDESCRIPTION as reason_description,\n",
    "            p.GENDER as patient_gender,\n",
    "            p.BIRTHDATE as patient_birthdate,\n",
    "            year(m.START) - year(p.BIRTHDATE) as patient_age_at_prescription,\n",
    "            p.STATE as patient_state,\n",
    "            p.ZIP as patient_zip,\n",
    "            e.ENCOUNTERCLASS as encounter_class,\n",
    "            e.PROVIDER as provider_id,\n",
    "            e.ORGANIZATION as organization_id,\n",
    "            prov.SPECIALITY as provider_specialty,\n",
    "            prov.NAME as provider_name,\n",
    "            m.ingest_run_id,\n",
    "            m.ingest_timestamp,\n",
    "            current_timestamp() as silver_load_timestamp\n",
    "        FROM {catalog_name}.synthea.medications_bronze m\n",
    "        LEFT JOIN {catalog_name}.synthea.patients_bronze p \n",
    "            ON m.PATIENT = p.Id\n",
    "        LEFT JOIN {catalog_name}.synthea.encounters_bronze e \n",
    "            ON m.ENCOUNTER = e.Id\n",
    "        LEFT JOIN {catalog_name}.synthea.providers_bronze prov \n",
    "            ON e.PROVIDER = prov.Id\n",
    "        WHERE m.ingest_run_id IN ('{run_ids_str}')\n",
    "    \"\"\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "    # Get parameters from Databricks widgets (for notebook tasks)\n",
    "    try:\n",
    "        from pyspark.dbutils import DBUtils\n",
    "        dbutils = DBUtils(spark)\n",
    "        catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "        load_type = dbutils.widgets.get(\"load_type\") if dbutils.widgets.get(\"load_type\") else \"full\"\n",
    "    except Exception:\n",
    "        # Fallback to argparse for Python script tasks\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--catalog_name\", type=str, required=True)\n",
    "        parser.add_argument(\"--load_type\", type=str, default=\"full\", choices=[\"full\", \"incremental\"])\n",
    "        args, _ = parser.parse_known_args()\n",
    "        catalog_name = args.catalog_name\n",
    "        load_type = args.load_type\n",
    "    table_name = \"medications_silver\"\n",
    "    \n",
    "    # Get run IDs to process based on load type\n",
    "    if load_type == \"full\":\n",
    "        print(\"Running FULL load - processing all data\")\n",
    "        run_ids = get_all_run_ids(spark, catalog_name, \"medications_bronze\")\n",
    "    else:\n",
    "        # Incremental load\n",
    "        last_run_id = get_last_processed_run_id(spark, catalog_name, table_name)\n",
    "        print(f\"Running INCREMENTAL load - Last processed run ID: {last_run_id}\")\n",
    "        run_ids = get_new_run_ids(spark, catalog_name, \"medications_bronze\", last_run_id)\n",
    "    \n",
    "    if not run_ids:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(run_ids)} run(s): {run_ids}\")\n",
    "    \n",
    "    df = load_fact_medications_incremental(spark, catalog_name, run_ids)\n",
    "    \n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.synthea\")\n",
    "    \n",
    "    # Write to silver table - use overwrite for full loads, append for incremental\n",
    "    write_mode = \"overwrite\" if load_type == \"full\" else \"append\"\n",
    "    (df.write\n",
    "        .mode(write_mode)\n",
    "        .format(\"delta\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(f\"{catalog_name}.synthea.{table_name}\"))\n",
    "    \n",
    "    update_last_processed_run_id(spark, catalog_name, table_name, run_ids[-1])\n",
    "    \n",
    "    record_count = df.count()\n",
    "    print(f\"\u2713 Loaded {record_count} records into {table_name} (mode: {load_type})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {},
  "databricks": {
   "environment": {
    "client": "4",
    "environmentId": "serverless_environment_demo"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}