{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import uuid\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_param_or_widget(spark: SparkSession, key: str, default: str | None = None, cli_args: dict[str, str] | None = None) -> str:\n",
    "    # Priority: CLI args > spark conf > widgets > default\n",
    "    if cli_args and key in cli_args and cli_args[key]:\n",
    "        return cli_args[key]\n",
    "    val = spark.conf.get(f\"job.param.{key}\", None)\n",
    "    if val is not None and val != \"\":\n",
    "        return val\n",
    "    try:\n",
    "        from pyspark.dbutils import DBUtils  # type: ignore\n",
    "\n",
    "        dbutils = DBUtils(spark)\n",
    "        w = dbutils.widgets.get(key)\n",
    "        if w:\n",
    "            return w\n",
    "    except Exception:\n",
    "        pass\n",
    "    if default is None:\n",
    "        raise ValueError(f\"Missing required parameter: {key}\")\n",
    "    return default\n",
    "\n",
    "\n",
    "def list_folders() -> list[str]:\n",
    "    return [\n",
    "        \"allergies\",\n",
    "        \"careplans\",\n",
    "        \"claims\",\n",
    "        \"claims_transactions\",\n",
    "        \"conditions\",\n",
    "        \"devices\",\n",
    "        \"encounters\",\n",
    "        \"imaging_studies\",\n",
    "        \"immunizations\",\n",
    "        \"medications\",\n",
    "        \"observations\",\n",
    "        \"organizations\",\n",
    "        \"patients\",\n",
    "        \"payer_transitions\",\n",
    "        \"payers\",\n",
    "        \"procedures\",\n",
    "        \"providers\",\n",
    "        \"supplies\",\n",
    "    ]\n",
    "\n",
    "\n",
    "def read_csv_with_metadata(spark: SparkSession, path: str, run_id: str):\n",
    "    df = (\n",
    "        spark.read.option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(path)\n",
    "        .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"source_file\", F.input_file_name())\n",
    "        .withColumn(\"ingest_run_id\", F.lit(run_id))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_bronze(df, full_table_name: str):\n",
    "    (\n",
    "        df.write.mode(\"append\")\n",
    "        .format(\"delta\")\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .saveAsTable(full_table_name)\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--catalog_name\", type=str, required=False)\n",
    "    parser.add_argument(\"--base_volume_path\", type=str, required=False)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    cli_args = {\n",
    "        \"catalog_name\": args.catalog_name or \"\",\n",
    "        \"base_volume_path\": args.base_volume_path or \"\",\n",
    "    }\n",
    "\n",
    "    catalog_name = get_param_or_widget(spark, \"catalog_name\", cli_args=cli_args)\n",
    "    base_volume_path = get_param_or_widget(spark, \"base_volume_path\", cli_args=cli_args)\n",
    "\n",
    "    run_id = str(uuid.uuid4())\n",
    "\n",
    "    for folder in list_folders():\n",
    "        landing_dir = f\"{base_volume_path.rstrip('/')}/{folder}\"\n",
    "        table_name = f\"{catalog_name}.synthea.{folder}_bronze\"\n",
    "\n",
    "        # Read all CSVs directly under the folder (exclude archive)\n",
    "        csv_glob = landing_dir + \"/*.csv\"\n",
    "        try:\n",
    "            df = read_csv_with_metadata(spark, csv_glob, run_id)\n",
    "        except Exception:\n",
    "            # If no files present, skip quietly\n",
    "            continue\n",
    "\n",
    "        # Create database if needed\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.synthea\")\n",
    "        write_bronze(df, table_name)\n",
    "        \n",
    "        print(f\"âœ“ Ingested data from {folder} into {table_name}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Volumes/maggiedatabricksterraform_dbw/synthea/admin_configs/requirements.txt"
    ],
    "environment_version": "4"
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
