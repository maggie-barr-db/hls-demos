{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import argparse\n",
        "\n",
        "\n",
        "def get_param_or_widget(spark: SparkSession, key: str, default: str | None = None, cli_args: dict[str, str] | None = None) -> str:\n",
        "    # Priority: CLI args > spark conf > widgets > default\n",
        "    if cli_args and key in cli_args and cli_args[key]:\n",
        "        return cli_args[key]\n",
        "    val = spark.conf.get(f\"job.param.{key}\", None)\n",
        "    if val is not None and val != \"\":\n",
        "        return val\n",
        "    try:\n",
        "        from pyspark.dbutils import DBUtils  # type: ignore\n",
        "\n",
        "        dbutils = DBUtils(spark)\n",
        "        w = dbutils.widgets.get(key)\n",
        "        if w:\n",
        "            return w\n",
        "    except Exception:\n",
        "        pass\n",
        "    if default is None:\n",
        "        raise ValueError(f\"Missing required parameter: {key}\")\n",
        "    return default\n",
        "\n",
        "\n",
        "def list_folders() -> list[str]:\n",
        "    return [\n",
        "        \"allergies\",\n",
        "        \"careplans\",\n",
        "        \"claims\",\n",
        "        \"claims_transactions\",\n",
        "        \"conditions\",\n",
        "        \"devices\",\n",
        "        \"encounters\",\n",
        "        \"imaging_studies\",\n",
        "        \"immunizations\",\n",
        "        \"medications\",\n",
        "        \"observations\",\n",
        "        \"organizations\",\n",
        "        \"patients\",\n",
        "        \"payer_transitions\",\n",
        "        \"payers\",\n",
        "        \"procedures\",\n",
        "        \"providers\",\n",
        "        \"supplies\",\n",
        "    ]\n",
        "\n",
        "\n",
        "def move_to_archive(spark: SparkSession, folder_path: str):\n",
        "    try:\n",
        "        from pyspark.dbutils import DBUtils  # type: ignore\n",
        "\n",
        "        dbutils = DBUtils(spark)\n",
        "    except Exception:\n",
        "        raise RuntimeError(\"dbutils is required to move files to archive\")\n",
        "\n",
        "    files = dbutils.fs.ls(folder_path)\n",
        "    archive_dir = folder_path.rstrip(\"/\") + \"/archive\"\n",
        "    try:\n",
        "        dbutils.fs.mkdirs(archive_dir)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    moved_count = 0\n",
        "    for f in files:\n",
        "        # Only move files (not directories) and skip archive itself\n",
        "        if f.path.endswith(\"/archive/\"):\n",
        "            continue\n",
        "        if f.path.endswith(\"/\"):\n",
        "            continue\n",
        "        target = archive_dir + \"/\" + f.name\n",
        "        dbutils.fs.mv(f.path, target, True)\n",
        "        moved_count += 1\n",
        "    \n",
        "    return moved_count\n",
        "\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--base_volume_path\", type=str, required=False)\n",
        "    args, _ = parser.parse_known_args()\n",
        "    cli_args = {\n",
        "        \"base_volume_path\": args.base_volume_path or \"\",\n",
        "    }\n",
        "\n",
        "    base_volume_path = get_param_or_widget(spark, \"base_volume_path\", cli_args=cli_args)\n",
        "\n",
        "    total_moved = 0\n",
        "    for folder in list_folders():\n",
        "        landing_dir = f\"{base_volume_path.rstrip('/')}/{folder}\"\n",
        "        try:\n",
        "            moved = move_to_archive(spark, landing_dir)\n",
        "            if moved > 0:\n",
        "                print(f\"✓ Archived {moved} file(s) from {folder}\")\n",
        "                total_moved += moved\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Warning: Could not archive files from {folder}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\nTotal files archived: {total_moved}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "databricks": {
      "environment": {
        "environmentId": "serverless_environment_demo"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
