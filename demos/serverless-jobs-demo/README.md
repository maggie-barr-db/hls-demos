# Serverless Jobs Demo

A comprehensive demonstration of Databricks serverless compute for jobs, showcasing the differences between classic and serverless architectures.

## ğŸ“ Project Structure

```
serverless-jobs-demo/
â”œâ”€â”€ config/                      # Environment configurations
â”‚   â”œâ”€â”€ variables.json           # Default/local config
â”‚   â”œâ”€â”€ variables.dev.json       # Development environment
â”‚   â”œâ”€â”€ variables.uat.json       # UAT environment
â”‚   â””â”€â”€ variables.prod.json      # Production environment
â”‚
â”œâ”€â”€ deployment/                  # Deployment scripts and docs
â”‚   â”œâ”€â”€ deploy_jobs.sh          # Multi-environment job deployment
â”‚   â”œâ”€â”€ deploy_all.sh           # Full stack deployment
â”‚   â””â”€â”€ README.md               # Detailed deployment guide
â”‚
â”œâ”€â”€ infrastructure/             # Databricks infrastructure
â”‚   â”œâ”€â”€ api_jobs/              # Job definitions (JSON)
â”‚   â”œâ”€â”€ init_scripts/          # Cluster init scripts
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â””â”€â”€ setup_volumes.sql      # Unity Catalog setup
â”‚
â””â”€â”€ src/                       # Source code
    â”œâ”€â”€ classic/               # Classic compute implementations
    â”‚   â”œâ”€â”€ bronze/           # Bronze layer (notebooks + scripts)
    â”‚   â”œâ”€â”€ silver/           # Silver layer (notebooks + scripts)
    â”‚   â”œâ”€â”€ gold/             # Gold layer (notebooks + scripts)
    â”‚   â””â”€â”€ utils/            # Shared utilities
    â”‚
    â””â”€â”€ serverless/           # Serverless compute implementations
        â”œâ”€â”€ bronze/           # Bronze layer (notebooks + scripts)
        â”œâ”€â”€ silver/           # Silver layer (notebooks + scripts)
        â”œâ”€â”€ gold/             # Gold layer (notebooks + scripts)
        â””â”€â”€ utils/            # Shared utilities
```

## ğŸš€ Quick Start

### 1. Configure Your Environment

Create/edit `config/variables.json` with your settings:

```json
{
  "catalog_name": "your_catalog",
  "base_volume_path": "/Volumes/your_catalog/synthea/landing",
  "admin_volume_path": "/Volumes/your_catalog/synthea/admin_configs"
}
```

### 2. Deploy

```bash
# Deploy everything (volumes, code, jobs)
cd deployment
./deploy_all.sh

# Or deploy just jobs to a specific environment
./deploy_jobs.sh prod
```

## ğŸ“š Documentation

- **[Deployment Guide](deployment/README.md)** - Complete deployment instructions and multi-environment setup
- **Job Definitions** - See `infrastructure/api_jobs/` for JSON job configurations

## ğŸ—ï¸ Architecture

### Classic vs Serverless

This demo showcases both **classic compute** and **serverless compute** approaches:

| Feature | Classic | Serverless |
|---------|---------|------------|
| **Startup Time** | Minutes | Seconds |
| **Libraries** | Init scripts + volume mounting | Environment definitions |
| **Config** | Environment variables | Job parameters |
| **Cost Model** | Per-cluster | Per-task execution |
| **Scaling** | Manual configuration | Automatic |

### Job Types

**Bronze Jobs** - Data ingestion (mixed: notebook + Python)
- `daily_bronze_ingestion_incr_classic_api` - Classic compute
- `daily_bronze_ingestion_incr_serverless_api` - Serverless compute

**Silver Jobs** - Data transformation (mixed: notebooks + Python)
- `daily_silver_load_incr_classic_api` - Classic compute  
- `daily_silver_load_incr_serverless_api` - Serverless compute

## ğŸ”§ Configuration Management

The project uses template placeholders for multi-environment deployment:

```json
{
  "spark_env_vars": {
    "catalog_name": "{{CATALOG_NAME}}"
  },
  "init_scripts": [{
    "volumes": {
      "destination": "/Volumes/{{CATALOG_NAME}}/synthea/admin_configs/install_faker_wheel.sh"
    }
  }]
}
```

At deployment time, `{{CATALOG_NAME}}` is replaced with environment-specific values.

## ğŸŒ Multi-Environment Support

Deploy to different environments without code changes:

```bash
cd deployment

# Development
./deploy_jobs.sh dev

# UAT
./deploy_jobs.sh uat

# Production
./deploy_jobs.sh prod
```

Each environment uses its own configuration file in `config/`.

## ğŸ“‹ Prerequisites

- Databricks workspace with Unity Catalog enabled
- Databricks CLI configured (`databricks configure`)
- Unity Catalog with:
  - Schema: `synthea`
  - Volumes: `landing`, `admin_configs`
- Python 3.x with `jq` installed

## ğŸ”— Related Resources

- [Databricks Serverless Compute](https://docs.databricks.com/serverless-compute/index.html)
- [Unity Catalog Volumes](https://docs.databricks.com/data-governance/unity-catalog/volumes.html)
- [Jobs API 2.1](https://docs.databricks.com/api/workspace/jobs)

## ğŸ“ License

Internal demo for customer engagements.

